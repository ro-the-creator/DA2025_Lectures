{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mod 5 Lecture 2 Code-Along:  Feature Engineering & Scaling \n",
    "\n",
    "### Goals\n",
    "* Create `hour_of_day` and `is_weekend` if not already done\n",
    "\n",
    "* Create `night_weekend_interaction` = `is_weekend * is_night`\n",
    "\n",
    "* Scale `hour_of_day` and `response_time_hrs` using both techniques (StandardScalar & MinMax)\n",
    "\n",
    "### Data\n",
    "Using the same NYC 311 dataset (remember the data is HUGE so we extracted just a week).  Data information exists [HERE](https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9/about_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data nyc311.csv \n",
    "\n",
    "df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell without changes!  You've done this in the previous Data Challenge \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "LOCAL_TZ = \"America/New_York\"\n",
    "\n",
    "def to_utc(series, local_tz=LOCAL_TZ):\n",
    "    \"\"\"\n",
    "    Idempotent conversion:\n",
    "      1) Parse to datetime.\n",
    "      2) If naive -> localize to local_tz (handle DST).\n",
    "      3) Convert to UTC.\n",
    "    Safe to re-run without raising 'Already tz-aware' errors.\n",
    "    \"\"\"\n",
    "    s = pd.to_datetime(series, errors=\"coerce\")\n",
    "\n",
    "    # if tz-naive, localize; if tz-aware, leave as-is\n",
    "    if s.dt.tz is None:\n",
    "        s = s.dt.tz_localize(local_tz, nonexistent=\"shift_forward\", ambiguous=\"NaT\")\n",
    "\n",
    "    return s.dt.tz_convert(\"UTC\")\n",
    "\n",
    "# --- Apply to your DataFrame (df) ---\n",
    "# Ensure the columns exist; adjust names if your file uses different headers\n",
    "required_cols = [\"Created Date\", \"Closed Date\"]\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "# Optionally drop rows that lack either timestamp before conversion\n",
    "df = df.dropna(subset=[\"Created Date\", \"Closed Date\"]).copy()\n",
    "\n",
    "df[\"Created Date\"] = to_utc(df[\"Created Date\"])\n",
    "df[\"Closed Date\"]  = to_utc(df[\"Closed Date\"])\n",
    "\n",
    "# Compute response time in hours\n",
    "delta = df[\"Closed Date\"] - df[\"Created Date\"]\n",
    "df[\"response_time_hrs\"] = delta.dt.total_seconds() / 3600\n",
    "\n",
    "# Drop any rows that became NaT due to ambiguous DST cases\n",
    "df = df.dropna(subset=[\"Created Date\", \"Closed Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1:  Create Features \n",
    "\n",
    "Extract the hour and create a variable for weekends (we done this previously!). We will define “night” as any time from midnight to 6am."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base features\n",
    "df['hour_of_day'] = None\n",
    "df['is_weekend'] = None\n",
    "df['is_night'] = None # 12am to 6am"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2:  Create Interaction Term \n",
    "\n",
    "Create the `night_weekend_interaction` feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['night_weekend_interaction'] = None\n",
    "\n",
    "#Look at the data -- so many columns in the data so only showing the ones we need \n",
    "df[['hour_of_day', 'is_weekend', 'is_night', 'night_weekend_interaction']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:  Scale Data \n",
    "* Use sklearn's StandardScaler object to scale hours and response time \n",
    "* Use sklearn's MinMaxScaler object to scale hours and response time \n",
    "\n",
    "**Note:  You will scale data before modeling in Mod 6; however, it will look slightly different because you will only scale a subset of the data (which we call \"training data\") vs. the whole dataset like we do here.  This is an important note!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"response_time_hrs\", \"hour_of_day\"])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df['hour_scaled'] = scaler.fit_transform(df[['hour_of_day']])\n",
    "df['resp_scaled'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell without changes -- do you see the difference in the scaled column? \n",
    "\n",
    "df[['resp_scaled', 'response_time_hrs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler()\n",
    "df[\"hour_mm\"] = minmax.fit_transform(df[[\"hour_of_day\"]])\n",
    "df[\"resp_mm\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell without changes -- do you see the difference in all 3 of the scaled columns? \n",
    "\n",
    "df[['resp_mm','resp_scaled', 'response_time_hrs',]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics Review with Jelly (Weeks 1 & 2)\n",
    "\n",
    "### Topics Covered \n",
    "* Probability \n",
    "* Bayes' Theorem \n",
    "* CDF \n",
    "* Welch t-test (sometimes Welch's)\n",
    "* ANOVA \n",
    "* Confidence Intervals \n",
    "\n",
    "### Data \n",
    "Student Performance on Exams from [Kaggle](https://www.kaggle.com/datasets/whenamancodes/students-performance-in-exams/data) :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 0 -- Read in Data get some info\n",
    "\n",
    "df = None\n",
    "\n",
    "#Info method\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA and Data Cleaning ALWAYS happens :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 -- Feature Engineering/A Lil' Cleaning \n",
    "\n",
    "df.columns = (\n",
    "    df.columns.str.lower()\n",
    "      .str.replace(r\"[^a-z0-9]+\",\"_\", regex=True)\n",
    "      .str.strip(\"_\")\n",
    ")\n",
    "\n",
    "#Create an overall score that adds all the scores together \n",
    "df[\"overall_score\"] = None\n",
    "\n",
    "#Create a passed variable that is binary \n",
    "PASS_CUT = 70\n",
    "df[\"passed\"] = None\n",
    "\n",
    "#Lowercase the 'completion' level within the test_preparation_course variable\n",
    "df[\"prep_completed\"] = (df[\"test_preparation_course\"].str.lower() == \"completed\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Rules you'll actually use \n",
    "Complement: if 72% pass, then 28% don’t (you’ll use this constantly when interpreting metrics).\n",
    "\n",
    "Addition/Multiplication: when to add vs multiply; independence warning.\n",
    "\n",
    "**From data: compute P(pass), P(prep), P(pass ∩ prep), P(pass | prep).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 -- Probability/A Lil' Bayes\n",
    "\n",
    "#Function to get means of our boolean values \n",
    "def p(s): return s.mean()\n",
    "\n",
    "#Probability of students who passed \n",
    "p_pass = p(df[\"passed\"])\n",
    "\n",
    "#Probability of students who completed prep\n",
    "p_prep = None\n",
    "\n",
    "#Probability of students who passed AND completed prep\n",
    "p_pass_and_prep = None\n",
    "\n",
    "#Probability of NOT passing \n",
    "p_not_pass = None\n",
    "\n",
    "#Probability a student will pass given they have prepped \n",
    "p_pass_given_prep = p_pass_and_prep / p_prep\n",
    "\n",
    "print(f\"P(pass) = {p_pass:.3f}\")\n",
    "print(f\"P(prep) = {p_prep:.3f}\")\n",
    "print(f\"P(pass ∩ prep) = {p_pass_and_prep:.3f}\")\n",
    "print(f\"P(pass | prep) = {p_pass_given_prep:.3f}\")\n",
    "print(f\"P(not pass) = {p_not_pass:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' theorem = updated belief after evidence \n",
    "\n",
    "Your prior = baseline pass rate; evidence = who completed prep; posterior = P(pass | prep).\n",
    "\n",
    "Compare both the (direct conditional from data) and the Bayes calculation below (prior, sensitivity, false-positive) -- should get the same answer! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3 -- More Bayes\n",
    "\n",
    "#Given our historical data, what is the probability a student passes if they completed the prep course\n",
    "\n",
    "# Prior Prob:  What we know from the data \n",
    "prior = p_pass\n",
    "tpr  = p(df.loc[df[\"passed\"], \"prep_completed\"])   # P(prep|pass)\n",
    "fpr   = None  # P(prep|not pass)\n",
    "\n",
    "def bayes_posterior(prior, p_B_given_A, p_B_given_notA):\n",
    "    return (p_B_given_A*prior) / (p_B_given_A*prior + p_B_given_notA*(1-prior))\n",
    "posterior_pass_given_prep = bayes_posterior(prior, tpr, fpr)\n",
    "\n",
    "print(f\"Bayes posterior P(pass | prep) = {posterior_pass_given_prep:.3f}\")\n",
    "#Check against the answer you got in the previous cell \n",
    "print(f\"Direct conditional check = {p_pass_given_prep:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF vs CDF: how to read AND use them with data \n",
    "\n",
    "PDF (hist density) tells you the shape and “where values live.”\n",
    "\n",
    "CDF tells you P(X ≤ x)—the probability up to a threshold (e.g., “% of students scoring ≤ 80”).\n",
    "\n",
    "Subtract the CDF from 1 to get the probability of a student having a score greater than 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) PDF vs CDF (math_score)\n",
    "x = df[\"math_score\"].astype(float)\n",
    "BINS = 15\n",
    "hist_vals, bin_edges = np.histogram(x, bins=BINS, density=True)\n",
    "#Get the probability of a math score of 80\n",
    "TARGET = 80\n",
    "cdf_at_target = (x <= TARGET).mean()\n",
    "\n",
    "print(f\"CDF at {TARGET} (P(X ≤ {TARGET})) = {cdf_at_target:.3f}\")\n",
    "#PDF (hist density) ≈ “relative likelihood around x”, CDF = P(X ≤ x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welch's t-test: Most common two-sample comparison in the real world\n",
    "\n",
    "Question: “Did prep help math scores?” → compare prep_completed vs not.\n",
    "\n",
    "Why Welch: you rarely know/assume equal variances in the \"wild\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Welch's t-test (difference in mean math score for prepping vs no prep)\n",
    "alpha = 0.05\n",
    "a = df.loc[df[\"prep_completed\"], \"math_score\"].astype(float)\n",
    "b = df.loc[~df[\"prep_completed\"], \"math_score\"].astype(float)\n",
    "tt = None\n",
    "print(f\"t = {tt.statistic:.3f}, p = {tt.pvalue:.4f}, reject H0 @ {alpha}? {tt.pvalue < alpha}\")\n",
    "print(f\"Group means: completed={a.mean():.1f}, none={b.mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-way ANOVA for 3+ groups \n",
    "\n",
    "Use a categorical segment (e.g., “race_ethnicity” in the file, or swap to “cohort/region” in your product data) to test “at least one mean differs.”\n",
    "\n",
    "- H0: All group means are the same\n",
    "- HA: A sig difference in the mean does exist in **at least one group**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Is the mean math score different across all levels of race_ethnicity\n",
    "groups = [g[\"math_score\"].astype(float).values for _, g in df.groupby(\"race_ethnicity\")]\n",
    "anova = None\n",
    "print(f\"ANOVA F = {anova.statistic:.3f}, p = {anova.pvalue:.4f}, reject H0? {anova.pvalue < alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Intervals you'll report all the time\n",
    "\n",
    "Mean CI (t-based): center ± t*SE for math_score.\n",
    "\n",
    "Proportion CI (Wilson): for passed. This is the CI you’ll use when reporting conversion rates.\n",
    "    - \"We're ~95% confident the true pop conversion rate is between LL and UL\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Confidence Intervals --  \n",
    "\n",
    "alpha = 0.05  # or reuse your existing alpha\n",
    "\n",
    "# Define x here so the cell is standalone\n",
    "x = pd.to_numeric(df[\"math_score\"], errors=\"coerce\").dropna()\n",
    "\n",
    "n = x.size\n",
    "xbar = x.mean()\n",
    "s = x.std(ddof=1)\n",
    "se = s / np.sqrt(n)\n",
    "tcrit = None\n",
    "mean_ci = None\n",
    "print(f\"Mean(math) = {xbar:.2f} ; {int((1-alpha)*100)}% CI = ({mean_ci[0]:.2f}, {mean_ci[1]:.2f})\")\n",
    "\n",
    "\n",
    "# Proportion CI for 'passed'\n",
    "\n",
    "valid = df[\"passed\"].notna()\n",
    "clicks = df.loc[valid, \"passed\"].sum()\n",
    "nobs = valid.sum()\n",
    "prop_ci = proportion_confint(count=clicks, nobs=nobs, alpha=alpha, method=\"wilson\")\n",
    "print(f\"Prop(pass) = {clicks/nobs:.3f} ; {int((1-alpha)*100)}% Wilson CI = ({prop_ci[0]:.3f}, {prop_ci[1]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' Theorem Steps (simulating DataCamp) -- ADVANCED \n",
    "\n",
    "- Filters the subgroup: uses only rows where prep_completed == True and looks at passed (Boolean) to model the true pass rate for this subgroup.\n",
    "\n",
    "- Splits into two batches: picks counts k1/n1 and k2/n2 to simulate “first study” then “follow-up study.”\n",
    "\n",
    "- Builds a parameter grid: creates a fine grid of θ values from 0.001 to 0.999 representing possible true pass rates.\n",
    "\n",
    "- Computes likelihoods: for each θ, evaluates the Binomial likelihood \n",
    "\n",
    "- Forms the old posterior: multiplies a uniform prior by Batch-1 likelihood and normalizes so the density integrates to 1.\n",
    "\n",
    "- Updates to the new posterior: uses the old posterior as the new prior, multiplies by Batch-2 likelihood, and normalizes again.\n",
    "\n",
    "- Summarizes beliefs: reports MAP (peak), posterior mean, and the 95% credible interval from the cumulative density.\n",
    "\n",
    "- Plots both posteriors: draws old posterior (becomes the new prior) and new posterior so you can visually compare the **shift (updated belief) and width (uncertainty).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' Theorem Key Terms:\n",
    "\n",
    "- pass_rate_grid: a list of possible true pass rates (0% to 100%). We test each candidate and see how plausible it is given the data.\n",
    "\n",
    "- prior (before data): our starting belief about the pass rate. We use a flat prior to start neutral.\n",
    "\n",
    "- likelihood: “If the true pass rate were r, how likely is it to see k passes out of n?” (Binomial formula).\n",
    "\n",
    "- posterior (after data): updated belief = prior × likelihood, then normalize so the curve sums to 1 (we divide by the total area using a simple sum).\n",
    "\n",
    "- old → new: the old posterior (after batch 1) becomes the new prior for batch 2, then we update again to get the new posterior.\n",
    "\n",
    "- MAP: the pass rate where the posterior is highest (the peak of the curve).\n",
    "\n",
    "- Posterior mean: the average of the whole curve (a smoothed estimate).\n",
    "\n",
    "- 95% credible interval: the middle 95% of our belief mass. “Given the data and our prior, we think the true pass rate is in this range 95% of the time.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Choose two batches from the prep-completed subgroup\n",
    "n1, n2 = 10, 12\n",
    "g = df.loc[df[\"prep_completed\"], \"passed\"].dropna().sample(frac=1, random_state=42)\n",
    "k1 = int(g.iloc[:n1].sum())                  # successes in batch 1\n",
    "k2 = int(g.iloc[n1:n1+n2].sum())             # successes in batch 2\n",
    "print(f\"Batch1: {k1}/{n1}  |  Batch2: {k2}/{n2}\")\n",
    "\n",
    "# 2) Build a grid of possible pass rates we want to consider (0%..100%)\n",
    "pass_rate_grid = np.linspace(0.001, 0.999, 1000)\n",
    "step = pass_rate_grid[1] - pass_rate_grid[0]  # spacing between grid points\n",
    "\n",
    "# 3) Function to update beliefs: prior × likelihood -> posterior, then normalize\n",
    "def update_posterior(prior_density, successes, trials):\n",
    "    # Binomial likelihood: \"How likely is what we saw if the true pass rate was r?\"\n",
    "    r = pass_rate_grid\n",
    "    likelihood = (r ** successes) * ((1 - r) ** (trials - successes))\n",
    "    unnormalized = prior_density * likelihood\n",
    "    area = unnormalized.sum() * step          # simple area under the curve\n",
    "    return unnormalized / area                # make it sum to 1 over the grid\n",
    "\n",
    "# Start with a flat (uniform) prior: we don't favor any pass rate at the start\n",
    "# All passing probs are equally likely (10%, 20%, 30% etc)\n",
    "prior_flat = np.ones_like(pass_rate_grid)\n",
    "\n",
    "# Old posterior (after Batch 1), then use it as the new prior for Batch 2\n",
    "posterior_old = update_posterior(prior_flat, k1, n1)          # after batch 1\n",
    "posterior_new = update_posterior(posterior_old, k2, n2)       # after batch 2\n",
    "\n",
    "# 4) Quick summaries: peak (MAP), average (mean), and 95% credible interval\n",
    "def summarize(density):\n",
    "    r = pass_rate_grid\n",
    "    map_est = r[np.argmax(density)]                           # most likely pass rate\n",
    "    mean_est = (r * density).sum() * step                     # average of the curve\n",
    "    cdf = np.cumsum(density) * step                           # running total (0..1)\n",
    "    lo = r[np.searchsorted(cdf, 0.025)]\n",
    "    hi = r[np.searchsorted(cdf, 0.975)]\n",
    "    return map_est, mean_est, (lo, hi)\n",
    "\n",
    "m1, mean1, ci1 = summarize(posterior_old)\n",
    "m2, mean2, ci2 = summarize(posterior_new)\n",
    "\n",
    "print(f\"Old posterior (after batch1): MAP={m1:.3f}, mean={mean1:.3f}, 95% CI=[{ci1[0]:.3f}, {ci1[1]:.3f}]\")\n",
    "print(f\"New posterior (after batch2): MAP={m2:.3f}, mean={mean2:.3f}, 95% CI=[{ci2[0]:.3f}, {ci2[1]:.3f}]\")\n",
    "\n",
    "# 5) DataFrames for seaborn (keyword args to avoid the positional-args error)\n",
    "df_old = pd.DataFrame({\"efficacy_rate\": pass_rate_grid, \"posterior_prob\": posterior_old})\n",
    "df_new = pd.DataFrame({\"efficacy_rate\": pass_rate_grid, \"new_posterior_prob\": posterior_new})\n",
    "\n",
    "sns.lineplot(data=df_new, x=\"efficacy_rate\", y=\"new_posterior_prob\", label=\"new posterior\")\n",
    "sns.lineplot(data=df_old, x=\"efficacy_rate\", y=\"posterior_prob\", label=\"old posterior = new prior\")\n",
    "plt.xlabel(\"Pass rate among prep-completed\"); plt.ylabel(\"Belief density\"); plt.title(\"Bayes (Grid) Posterior Update\")\n",
    "plt.legend(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Assignment Mod6L14 — Confusion Matrix & Metrics on Restaurant Inspections\n",
    "\n",
    "**Format:** Instructor Guidance → Your Task → We Share (Reflection)\n",
    "\n",
    "**Goal:** Using the same **classification workflow** and models as last time, build confusion matrices and calculate accuracy, precision, and recall of each model.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructor Guidance (Pseudocode + Docs)\n",
    "\n",
    "Use this as a roadmap; students implement below.\n",
    "\n",
    "**Docs (quick links):**\n",
    "- Train/Test Split — scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html  \n",
    "- Logistic Regression — scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html  \n",
    "- Confusion Matrix / Classification Report — scikit-learn: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics  \n",
    "- `pandas.get_dummies` (one-hot) — https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n",
    "\n",
    "### Guidance (same models as last challenge -- FEEL free to copy and paste the code that you need from the last code assignment)\n",
    "\n",
    "1) **Load CSV** → preview shape & columns.  \n",
    "2) **Define target (y)** as a **binary** label: e.g., `CRITICAL FLAG == \"Critical\"` → 1, else 0 *(positive class = “Critical”)*.  \n",
    "3) **Pick features (X)**: start small (`SCORE`), then add categorical dummies (`BORO`, `CUISINE DESCRIPTION`), keep it simple.  \n",
    "4) **Minimal prep**: coerce **only used columns** to numeric (for numeric features); `get_dummies` for categoricals (`drop_first=True`). Drop NA rows on used cols.  \n",
    "5) **Train–test split (80/20)** with fixed `random_state`.  \n",
    "6) **Fit the same model** as last time (e.g., `LogisticRegression`), predict on **test**.  \n",
    "7) **Evaluate**: confusion matrix, **accuracy**, **precision**, **recall**, **F1** (classification_report).  \n",
    "8) **Repeat for 2 more models** (A (baseline)/B (single term)/C (more than one term)) on the **same split**; compare metrics.  \n",
    "9) **Decide** which model is better for the business goal based on **precision vs recall** trade-offs.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task\n",
    "\n",
    "Work in pairs. Comment your choices briefly. Keep code simple and readable.\n",
    "\n",
    "> **Reminder (last challenge model):** You trained **LogisticRegression**, on three models you will use those same 3 models!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Read the Restaurant Inspection CSV & Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Define the Binary Target (y)\n",
    "\n",
    "We’ll predict whether an inspection had a **Critical violation**.\n",
    "\n",
    "- Target rule: `CRITICAL FLAG == \"Critical\"` → **1**, else **0**.\n",
    "- Positive class = **1 (Critical)** — keep this in mind for **precision/recall** meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Create Three Models (you can copy and paste the same modeling code from last code assignment here it is okay if your models have different features as long as you have 3 models)\n",
    "\n",
    "- **Model A (minimal numeric):** `SCORE` only (lower is better in NYC scoring).  \n",
    "- **Model B (add location):** `SCORE` + one-hot `BORO`.  \n",
    "- **Model C (richer categories):** `SCORE` + one-hot `BORO` + top cuisines (one-hot of `CUISINE DESCRIPTION` limited to most frequent K).\n",
    "\n",
    "> If any column is missing in your file, skip that spec or adjust accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Minimal Cleaning: Drop NAs in Used Columns\n",
    "\n",
    "Do this **per model** so each spec uses its own “clean” subset (same **target** slice).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Train–Test Split (same split for all models)\n",
    "\n",
    "Use **the same random_state** so models A/B/C are comparable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Fit the Same Models on the Train sets (Logistic Regression), Get Predictions, Print Confusion Matrices & Metric Reports\n",
    "\n",
    "> **This is new**:  Be sure to look up documentation on `confusion_matrix` and `classification_report`.  Get a matrix and metrics report (accuracy, precision, recall, f1) for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Visualize One Confusion Matrix \n",
    "\n",
    "Be able to interpret this Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Decide Which Model Is “Better” — Explain Your Metric Choice\n",
    "\n",
    "- If the **cost of missing a Critical** violation is high → prioritize **Recall** on the positive class.  \n",
    "- If the **cost of wrongly flagging Critical** is high → prioritize **Precision** on the positive class.  \n",
    "- If you want a balance → **F1**.\n",
    "\n",
    "Write 3–5 sentences justifying your pick using the table above (Precision/Recall/F1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We Share (Reflection)\n",
    "\n",
    "1) **Chosen model (A/B/C)** and **why**, referencing **Precision/Recall/F1** for the **Critical (1)** class.  \n",
    "2) Your **confusion matrix** and a one-liner in plain English:  \n",
    "   - “Out of all inspections we predicted **Critical**, **X%** were actually Critical (Precision).  \n",
    "   - Of all actually **Critical** inspections, we caught **Y%** (Recall).”  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Challenge 9 — Feature Engineering & Feature Selection\n",
    "\n",
    "**Format:** Instructor Guidance → You Do (Students) → We Share (Reflection)\n",
    "\n",
    "**Goal:** Engineer better predictors (one-hot/dummies, interactions, polynomials), avoid unnecessary complexity, and compare a **Base** vs **Engineered** model on the **same train–test split** using **MAE/RMSE**. Interpret coefficients in units and explain business value.\n",
    "\n",
    "\n",
    "\n",
    "> Dataset: **NYC Yellow Taxi — Dec 2023** (CSV). Keep code *simple*: light numeric coercion only for your chosen columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructor Guidance\n",
    "\n",
    "**Hint: Use the Lecture Deck, Canvas Reading, and Docs to help you with the code**\n",
    "\n",
    "**Docs (quick links):**\n",
    "- One-hot encoding (pandas): https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html  \n",
    "- OneHotEncoder (sklearn): https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html  \n",
    "- Train/Test Split: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html  \n",
    "- MAE / MSE / RMSE: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html  \n",
    "- OLS (statsmodels): https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html  \n",
    "- OLS Results (coef/p/CIs/resid): https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.RegressionResults.html\n",
    "\n",
    "### Pseudocode Plan (Feature Engineering + Selection)\n",
    "1) **Load CSV** → preview columns/shape.  \n",
    "2) **Pick Y and initial Xs (2–3 numeric)** → keep it simple and decision-time-available.  \n",
    "3) **Engineer features:**\n",
    "   - **One-hot** a categorical with a dropped baseline (e.g., `payment_type` or `weekday/weekend`).  \n",
    "   - **Interaction**: choose a hypothesis-driven pair (e.g., `trip_distance × is_weekend`).  \n",
    "   - **Polynomial**: add one squared term for a plausible curve (e.g., `trip_distance²`).  \n",
    "4) **Build Base vs Engineered design matrices** (add intercept).  \n",
    "5) **Single train–test split** (80/20, fixed `random_state`) shared by both models.  \n",
    "6) **Fit on TRAIN**, **predict on TEST** for both models; compute **MAE/RMSE** (units of Y).  \n",
    "7) **Interpretation**: write unit-based coefficient sentences; note baseline category for dummies.  \n",
    "8) **Light selection**: if Engineered model doesn’t beat Base on TEST (or adds complexity w/o value), prefer Base.  \n",
    "9) **Diagnostics (quick)**: residuals vs fitted (train); note any cones (heteroskedasticity).  \n",
    "10) **Stakeholder one-liner**: which model, why (TEST metrics in units), and what the added features *mean*.\n",
    "markdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Do — Student Section\n",
    "Work in pairs. Comment your choices briefly. Keep code simple—only coerce the columns you use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 — Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 — Load CSV & Preview\n",
    "- Point to your **Dec 2023** taxi CSV.\n",
    "- Print **shape** and **columns**.\n",
    "\n",
    "**Hint: You may have to drop missing values and do a force coercion to make sure the variables stay numeric (other coding assignments may help)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 —  Pick Target **Y** and Predictors **Xs** (choose 2–3 numeric)\n",
    "\n",
    "- **Avoid** using an X that directly defines Y (e.g., `total_amount` when Y = `fare_amount`).\n",
    "- Coerce **only these columns** to numeric; drop NA rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 —  Engineer New Features (One-hot, Interaction, Polynomial)\n",
    "\n",
    "Pick **one** categorical to one-hot (drop baseline). Options that usually exist:\n",
    "\n",
    "- `payment_type` (codes): treat as categorical strings for clarity, then one-hot with drop_first=True, or  \n",
    "- derive **weekday/weekend** from `tpep_pickup_datetime` if present.\n",
    "\n",
    "Then add **one interaction** and **one squared term** guided by a business hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 — Build **Base** and **Engineered** Design Matrices\n",
    "\n",
    "- **Base** = intercept + base predictors (Xs you assigned in Step 2) \n",
    "- **Engineered** = intercept + base predictors + engineered columns (dummies + interaction + polynomial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 — Single Train–Test Split (Shared by Both Models)\n",
    "\n",
    "Use one split so Base and Engineered are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 — Fit on TRAIN, Predict on TEST, Compute **MAE/RMSE** (units of Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 — Interpret Key Coefficients (Plain Language)\n",
    "\n",
    "Write **unit-based** interpretations for 2–3 impactful coefficients **in the Engineered model**, noting:\n",
    "- The **baseline** category for dummies (the dropped category).\n",
    "- **Interaction** meaning (change in slope under the condition).\n",
    "- **Polynomial** meaning (curve: does effect rise then taper?).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Use this template; edit to your variables/units):*\n",
    "\n",
    "- **Dummy (pay_…):** Compared to baseline **[dropped category]**, the expected **Y** is **β** higher/lower, holding other features constant.  \n",
    "- **Interaction (dist×weekend):** On weekends, each additional **mile** changes **Y** by **β_interaction** *more/less* than on weekdays, holding other features constant.  \n",
    "- **Polynomial (distance²):** The marginal effect of distance changes with distance; the negative/positive β on distance² indicates **diminishing/increasing** returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8 —  Quick Diagnostics (Train Residuals) — Engineered Model\n",
    "- **Residuals vs Fitted:** random cloud ≈ good; cone/funnel suggests non-constant variance.  \n",
    "- **Q–Q plot:** points roughly along diagonal (normality for inference).  \n",
    "- **Durbin–Watson:** printed in `eng_model.summary()` (~2 suggests independence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We Share — Reflection & Wrap‑Up\n",
    "\n",
    "**Notes on Feature Selection**\n",
    "- If **Engineered** doesn’t beat **Base** on TEST (or gains are tiny), prefer **Base** for simplicity.  \n",
    "- If two engineered features are redundant (e.g., highly correlated dummies), consider dropping one.  \n",
    "- Keep features that improve TEST error **and** you can explain to a stakeholder.\n",
    "\n",
    "\n",
    "Write **2 short paragraphs** and be specific:\n",
    "\n",
    "\n",
    "1) **Which model would you deploy today—Base or Engineered—and why?**  \n",
    "Use **TEST MAE/RMSE in units**, your coefficient interpretations (baseline/interaction/polynomial), and any residual observations.\n",
    "\n",
    "2) **What engineered feature was most useful (or not)?**  \n",
    "Explain the **business logic** behind it and whether it earned its place on the TEST set. If not, what would you try next (different interaction, different categorical, or simplifying features)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

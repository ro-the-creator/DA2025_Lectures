{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Challenge 9 — Feature Engineering & Feature Selection\n",
    "\n",
    "**Format:** Instructor Guidance → You Do (Students) → We Share (Reflection)\n",
    "\n",
    "**Goal:** Engineer better predictors (one-hot/dummies, interactions, polynomials), avoid unnecessary complexity, and compare a **Base** vs **Engineered** model on the **same train–test split** using **MAE/RMSE**. Interpret coefficients in units and explain business value.\n",
    "\n",
    "\n",
    "\n",
    "> Dataset: **NYC Yellow Taxi — Dec 2023** (CSV). Keep code *simple*: light numeric coercion only for your chosen columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructor Guidance\n",
    "\n",
    "**Hint: Use the Lecture Deck, Canvas Reading, and Docs to help you with the code**\n",
    "\n",
    "**Docs (quick links):**\n",
    "- One-hot encoding (pandas): https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html  \n",
    "- OneHotEncoder (sklearn): https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html  \n",
    "- Train/Test Split: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html  \n",
    "- MAE / MSE / RMSE: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html  \n",
    "- OLS (statsmodels): https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html  \n",
    "- OLS Results (coef/p/CIs/resid): https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.RegressionResults.html\n",
    "\n",
    "### Pseudocode Plan (Feature Engineering + Selection)\n",
    "1) **Load CSV** → preview columns/shape.  \n",
    "2) **Pick Y and initial Xs (2–3 numeric)** → keep it simple and decision-time-available.  \n",
    "3) **Engineer features:**\n",
    "   - **One-hot** a categorical with a dropped baseline (e.g., `payment_type` or `weekday/weekend`).  \n",
    "   - **Interaction**: choose a hypothesis-driven pair (e.g., `trip_distance × is_weekend`).  \n",
    "   - **Polynomial**: add one squared term for a plausible curve (e.g., `trip_distance²`).  \n",
    "4) **Build Base vs Engineered design matrices** (add intercept).  \n",
    "5) **Single train–test split** (80/20, fixed `random_state`) shared by both models.  \n",
    "6) **Fit on TRAIN**, **predict on TEST** for both models; compute **MAE/RMSE** (units of Y).  \n",
    "7) **Interpretation**: write unit-based coefficient sentences; note baseline category for dummies.  \n",
    "8) **Light selection**: if Engineered model doesn’t beat Base on TEST (or adds complexity w/o value), prefer Base.  \n",
    "9) **Diagnostics (quick)**: residuals vs fitted (train); note any cones (heteroskedasticity).  \n",
    "10) **Stakeholder one-liner**: which model, why (TEST metrics in units), and what the added features *mean*.\n",
    "markdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Do — Student Section\n",
    "Work in pairs. Comment your choices briefly. Keep code simple—only coerce the columns you use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 — Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 — Load CSV & Preview\n",
    "- Point to your **Dec 2023** taxi CSV.\n",
    "- Print **shape** and **columns**.\n",
    "\n",
    "**Hint: You may have to drop missing values and do a force coercion to make sure the variables stay numeric (other coding assignments may help)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gs/yv3dj4zx2tgdd5tw701ypspm0000gp/T/ipykernel_18887/44345454.py:2: DtypeWarning: Columns (4,10,13,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3310907, 19)\n",
      "Index(['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
      "       'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag',\n",
      "       'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra',\n",
      "       'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',\n",
      "       'total_amount', 'congestion_surcharge', 'airport_fee'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/Marcy_Student/Desktop/Marcy-Modules/marcy-git/DA2025_Lectures/Mod6/data/2023_Yellow_Taxi_Trip_Data_20251015.csv'\n",
    "df = pd.read_csv(path)\n",
    "print(df.shape)\n",
    "print(df.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 —  Pick Target **Y** and Predictors **Xs** (choose 2–3 numeric)\n",
    "\n",
    "- **Avoid** using an X that directly defines Y (e.g., `total_amount` when Y = `fare_amount`).\n",
    "- Coerce **only these columns** to numeric; drop NA rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coerce fare, tip, distance to numeric safely\n",
    "num_cols = ['fare_amount', 'tip_amount', 'trip_distance', 'passenger_count']\n",
    "for c in num_cols:\n",
    "    df[c] = pd.to_numeric(\n",
    "        df[c].astype(str).str.strip().str.replace(r'[^0-9.+\\-eE]', '', regex=True),\n",
    "        errors='coerce'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some filters\n",
    "df = df[(df['tip_amount']<= 1000) & (df['tip_amount'] > 0) & (df['trip_distance'] > 0) & (df['trip_distance'] <= 560) & (df['extra'] >= 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 —  Engineer New Features (One-hot, Interaction, Polynomial)\n",
    "\n",
    "Pick **one** categorical to one-hot (drop baseline). Options that usually exist:\n",
    "\n",
    "- `payment_type` (codes): treat as categorical strings for clarity, then one-hot with drop_first=True, or  \n",
    "- derive **weekday/weekend** from `tpep_pickup_datetime` if present.\n",
    "\n",
    "Then add **one interaction** and **one squared term** guided by a business hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data with sklearn.OneHotEncoder ---\n",
      "   VendorID    tpep_pickup_datetime   tpep_dropoff_datetime  passenger_count  \\\n",
      "0         2  12/01/2023 04:11:39 PM  12/01/2023 04:19:13 PM           2.0000   \n",
      "1         1  12/01/2023 04:11:39 PM  12/01/2023 04:34:39 PM           2.0000   \n",
      "2         2  12/01/2023 04:11:40 PM  12/01/2023 04:28:50 PM           6.0000   \n",
      "3         1  12/01/2023 04:11:41 PM  12/01/2023 04:14:35 PM           1.0000   \n",
      "4         2  12/01/2023 04:11:41 PM  12/01/2023 04:28:34 PM           1.0000   \n",
      "\n",
      "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
      "0         0.6900      1.0000                  N           141           140   \n",
      "1         3.0000      1.0000                  N           164           211   \n",
      "2         2.1500      1.0000                  N           238            48   \n",
      "3         0.3000      1.0000                  N           163           161   \n",
      "4         1.4700      1.0000                  N           137           229   \n",
      "\n",
      "   payment_type  ...  tip_amount  tolls_amount  improvement_surcharge  \\\n",
      "0             1  ...      3.0000        0.0000                 1.0000   \n",
      "1             1  ...      3.0000        0.0000                 1.0000   \n",
      "2             1  ...      3.4200        0.0000                 1.0000   \n",
      "3             1  ...      2.2000        0.0000                 1.0000   \n",
      "4             1  ...      3.7800        0.0000                 1.0000   \n",
      "\n",
      "   total_amount  congestion_surcharge  airport_fee payment_type_1  \\\n",
      "0       17.4000                2.5000       0.0000         1.0000   \n",
      "1       31.4000                2.5000       0.0000         1.0000   \n",
      "2       26.2200                2.5000       0.0000         1.0000   \n",
      "3       13.1000                2.5000       0.0000         1.0000   \n",
      "4       22.6800                2.5000       0.0000         1.0000   \n",
      "\n",
      "   payment_type_2  payment_type_3  payment_type_4  \n",
      "0          0.0000          0.0000          0.0000  \n",
      "1          0.0000          0.0000          0.0000  \n",
      "2          0.0000          0.0000          0.0000  \n",
      "3          0.0000          0.0000          0.0000  \n",
      "4          0.0000          0.0000          0.0000  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# not gonna use this for the One-hot feature, but practicing using OneHotEncoder()\n",
    "\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False)\n",
    "feature_array = ohe.fit_transform(df[['payment_type']])\n",
    "feature_labels = list(ohe.get_feature_names_out())\n",
    "dummies_skl = pd.DataFrame(feature_array, columns=feature_labels)\n",
    "\n",
    "# Concatenate back to the original dataframe\n",
    "df_with_skl = pd.concat([df.reset_index(drop=True), dummies_skl.reset_index(drop=True)], axis=1)\n",
    "print(\"\\n--- Data with sklearn.OneHotEncoder ---\")\n",
    "print(df_with_skl.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weekday\n",
       "5    409057\n",
       "4    391492\n",
       "3    361469\n",
       "2    352651\n",
       "6    334953\n",
       "1    330441\n",
       "0    265981\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot is_weekend\n",
    "df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "df['weekday'] = df['tpep_pickup_datetime'].dt.weekday\n",
    "\n",
    "df['weekday'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_weekend\n",
       "0    1702034\n",
       "1     744010\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weekend_flag = []\n",
    "\n",
    "for row in df['weekday']:\n",
    "    if row == 5 or row == 6:\n",
    "        weekend_flag.append(1)\n",
    "    else:\n",
    "        weekend_flag.append(0)\n",
    "\n",
    "df['is_weekend'] = weekend_flag\n",
    "df['is_weekend'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction is_weekend * trip_distance\n",
    "df['weekend_trip_distance'] = df['is_weekend'] * df['trip_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0.6900\n",
       "4          3.0000\n",
       "7          2.1500\n",
       "9          0.3000\n",
       "10         1.4700\n",
       "            ...  \n",
       "3310899    2.7000\n",
       "3310902    1.7000\n",
       "3310903   21.6000\n",
       "3310905    0.0100\n",
       "3310906   16.6700\n",
       "Name: trip_distance, Length: 2446044, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Polynomial term trip_distance squared\n",
    "df['trip_distance_sq']  = df['trip_distance']**2\n",
    "df['trip_distance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 — Build **Base** and **Engineered** Design Matrices\n",
    "\n",
    "- **Base** = intercept + base predictors (Xs you assigned in Step 2) \n",
    "- **Engineered** = intercept + base predictors + engineered columns (dummies + interaction + polynomial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df[['is_weekend', 'weekend_trip_distance']]\n",
    "X2 = df[['is_weekend', 'weekend_trip_distance', 'trip_distance_sq']]\n",
    "y = df['fare_amount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 — Single Train–Test Split (Shared by Both Models)\n",
    "\n",
    "Use one split so Base and Engineered are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X1, y, train_size=0.8, test_size=0.2, random_state=42)\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(X2, y, train_size=0.8, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 — Fit on TRAIN, Predict on TEST, Compute **MAE/RMSE** (units of Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 — Interpret Key Coefficients (Plain Language)\n",
    "\n",
    "Write **unit-based** interpretations for 2–3 impactful coefficients **in the Engineered model**, noting:\n",
    "- The **baseline** category for dummies (the dropped category).\n",
    "- **Interaction** meaning (change in slope under the condition).\n",
    "- **Polynomial** meaning (curve: does effect rise then taper?).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Use this template; edit to your variables/units):*\n",
    "\n",
    "- **Dummy (pay_…):** Compared to baseline **[dropped category]**, the expected **Y** is **β** higher/lower, holding other features constant.  \n",
    "- **Interaction (dist×weekend):** On weekends, each additional **mile** changes **Y** by **β_interaction** *more/less* than on weekdays, holding other features constant.  \n",
    "- **Polynomial (distance²):** The marginal effect of distance changes with distance; the negative/positive β on distance² indicates **diminishing/increasing** returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8 —  Quick Diagnostics (Train Residuals) — Engineered Model\n",
    "- **Residuals vs Fitted:** random cloud ≈ good; cone/funnel suggests non-constant variance.  \n",
    "- **Q–Q plot:** points roughly along diagonal (normality for inference).  \n",
    "- **Durbin–Watson:** printed in `eng_model.summary()` (~2 suggests independence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We Share — Reflection & Wrap‑Up\n",
    "\n",
    "**Notes on Feature Selection**\n",
    "- If **Engineered** doesn’t beat **Base** on TEST (or gains are tiny), prefer **Base** for simplicity.  \n",
    "- If two engineered features are redundant (e.g., highly correlated dummies), consider dropping one.  \n",
    "- Keep features that improve TEST error **and** you can explain to a stakeholder.\n",
    "\n",
    "\n",
    "Write **2 short paragraphs** and be specific:\n",
    "\n",
    "\n",
    "1) **Which model would you deploy today—Base or Engineered—and why?**  \n",
    "Use **TEST MAE/RMSE in units**, your coefficient interpretations (baseline/interaction/polynomial), and any residual observations.\n",
    "\n",
    "2) **What engineered feature was most useful (or not)?**  \n",
    "Explain the **business logic** behind it and whether it earned its place on the TEST set. If not, what would you try next (different interaction, different categorical, or simplifying features)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data with pd.get_dummies ---\n",
      "     sales  ad_spend  web_traffic day_type  ad_x_traffic  ad_spend_sq  \\\n",
      "0 102.2900    5.4881      33.8908  Weekend      185.9974      30.1196   \n",
      "1  62.3706    7.1519      13.5004  Weekday       96.5534      51.1496   \n",
      "2  91.0980    6.0276      36.7597  Weekend      221.5740      36.3324   \n",
      "3 110.9057    5.4488      48.1094  Weekend      262.1402      29.6898   \n",
      "4  50.8550    4.2365      12.4377  Weekend       52.6927      17.9483   \n",
      "\n",
      "   day_Weekend  \n",
      "0         True  \n",
      "1        False  \n",
      "2         True  \n",
      "3         True  \n",
      "4         True  \n",
      "\n",
      "--- Data with sklearn.OneHotEncoder ---\n",
      "     sales  ad_spend  web_traffic day_type  ad_x_traffic  ad_spend_sq  \\\n",
      "0 102.2900    5.4881      33.8908  Weekend      185.9974      30.1196   \n",
      "1  62.3706    7.1519      13.5004  Weekday       96.5534      51.1496   \n",
      "2  91.0980    6.0276      36.7597  Weekend      221.5740      36.3324   \n",
      "3 110.9057    5.4488      48.1094  Weekend      262.1402      29.6898   \n",
      "4  50.8550    4.2365      12.4377  Weekend       52.6927      17.9483   \n",
      "\n",
      "   day_type_Weekend  \n",
      "0            1.0000  \n",
      "1            0.0000  \n",
      "2            1.0000  \n",
      "3            1.0000  \n",
      "4            1.0000  \n"
     ]
    }
   ],
   "source": [
    "# First, recreate the DataFrame from Reading 9\n",
    "np.random.seed(0)\n",
    "ad_spend = np.random.rand(100) * 10\n",
    "web_traffic = np.random.rand(100) * 50\n",
    "sales = 15 + 5 * ad_spend + 1.5 * web_traffic + np.random.normal(0, 8, 100)\n",
    "df_mlr = pd.DataFrame({'sales': sales, 'ad_spend': ad_spend, 'web_traffic': web_traffic})\n",
    "df_mlr['day_type'] = np.random.choice(['Weekday', 'Weekend'], 100)\n",
    "\n",
    "# --- 1. Interaction Term ---\n",
    "df_mlr['ad_x_traffic'] = df_mlr['ad_spend'] * df_mlr['web_traffic']\n",
    "\n",
    "# --- 2. Squared Term (for non-linear effect) ---\n",
    "df_mlr['ad_spend_sq'] = df_mlr['ad_spend']**2\n",
    "\n",
    "# --- 3. Dummy Variables (Method A: pandas.get_dummies) ---\n",
    "# Easy for analysis, use drop_first=True\n",
    "dummies_pd = pd.get_dummies(df_mlr['day_type'], drop_first=True, prefix='day')\n",
    "df_with_pd = pd.concat([df_mlr, dummies_pd], axis=1)\n",
    "print(\"--- Data with pd.get_dummies ---\")\n",
    "print(df_with_pd.head())\n",
    "\n",
    "# --- 3. Dummy Variables (Method B: sklearn.OneHotEncoder) ---\n",
    "# Better for ML pipelines. Note: This is more complex to set up.\n",
    "# We fit the encoder on the 'day_type' column\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False)\n",
    "feature_array = ohe.fit_transform(df_mlr[['day_type']])\n",
    "feature_labels = list(ohe.get_feature_names_out())\n",
    "dummies_skl = pd.DataFrame(feature_array, columns=feature_labels)\n",
    "\n",
    "# Concatenate back to the original dataframe\n",
    "df_with_skl = pd.concat([df_mlr.reset_index(drop=True), dummies_skl.reset_index(drop=True)], axis=1)\n",
    "print(\"\\n--- Data with sklearn.OneHotEncoder ---\")\n",
    "print(df_with_skl.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
